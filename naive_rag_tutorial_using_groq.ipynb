{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8xbNFTWLUvY"
   },
   "source": [
    "# Building a Naive RAG Application with LangChain and Groq ðŸ“š\n",
    "\n",
    "This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) system using LangChain and Groq. We'll create a PDF question-answering system that can:\n",
    "1. Process PDF documents\n",
    "2. Generate embeddings\n",
    "3. Perform similarity search\n",
    "4. Generate context-aware responses\n",
    "\n",
    "## Prerequisites\n",
    "- A Groq API key (get one from [Groq Console](https://console.groq.com))\n",
    "- Python 3.9+\n",
    "- Google Colab or local Jupyter environment\n",
    "\n",
    "## Setup Instructions\n",
    "1. Set your Groq API key in the environment\n",
    "2. Install required packages\n",
    "3. Sample PDF document already provided as URL for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTYKMfFELUvb"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain-core langchain-community langchain-groq langchain-huggingface faiss-cpu pypdf python-dotenv requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bvHsE5S2LUvc"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "# LangChain components\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULuievVHOUEl"
   },
   "source": [
    "## Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1880,
     "status": "ok",
     "timestamp": 1739619975131,
     "user": {
      "displayName": "Rohit Khattar",
      "userId": "03191352284834786517"
     },
     "user_tz": -330
    },
    "id": "ZQKhOEBQOXNr",
    "outputId": "996af484-c5ef-4be5-bcd1-8a8349e619f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API keys have been set!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')\n",
    "\n",
    "print(\"API keys have been set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Q7fsKBQLUvc"
   },
   "source": [
    "## Step 1: Document Processing\n",
    "\n",
    "First, we'll create functions to load and process PDF documents. The process includes:\n",
    "1. Loading the PDF\n",
    "2. Splitting text into manageable chunks\n",
    "3. Creating embeddings and building a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJuH8tpKLUvc"
   },
   "outputs": [],
   "source": [
    "# def load_pdf(file_path: str) -> List[Document]:\n",
    "#     \"\"\"Load a PDF file and convert it to a list of documents.\"\"\"\n",
    "#     loader = PyPDFLoader(file_path)\n",
    "#     documents = loader.load()\n",
    "#     print(f\"Loaded PDF with {len(documents)} pages\")\n",
    "#     return documents\n",
    "\n",
    "def load_pdf(url: str) -> List[Document]:\n",
    "    \"\"\"Load a PDF file from a URL and convert it to a list of documents.\"\"\"\n",
    "    if not url.lower().endswith(\".pdf\"):\n",
    "        print(\"Error: Please provide a valid PDF URL ending with '.pdf'\")\n",
    "        return []\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Create a temporary file to store the PDF content\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as temp_file:\n",
    "        temp_file.write(response.content)\n",
    "        temp_file_path = temp_file.name\n",
    "\n",
    "    # Use the temporary file path with PyPDFLoader\n",
    "    loader = PyPDFLoader(temp_file_path)\n",
    "    documents = loader.load()\n",
    "    print(f\"Loaded PDF with {len(documents)} pages\")\n",
    "\n",
    "    # Remove the temporary file\n",
    "    os.remove(temp_file_path)\n",
    "\n",
    "    return documents\n",
    "\n",
    "def create_vector_store(documents: List[Document]) -> FAISS:\n",
    "    \"\"\"Create a FAISS vector store from documents.\"\"\"\n",
    "    # Initialize text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "    # Split documents into chunks\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split documents into {len(docs)} chunks\")\n",
    "\n",
    "    # Initialize embeddings (using HuggingFaceEmbeddings here)\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "    )\n",
    "\n",
    "    # Create vector store with FAISS\n",
    "    vector_store = FAISS.from_documents(docs, embeddings)\n",
    "    print(\"Vector store created successfully\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uh2zzWtTLUvd"
   },
   "source": [
    "## Step 2: Process a PDF\n",
    "\n",
    " Load a PDF document from a specified URL and create a vector store for efficient content retrieval.\n",
    "\n",
    "Provided the example PDF URL, you can change to play around with your specific PDF as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57757,
     "status": "ok",
     "timestamp": 1739620032885,
     "user": {
      "displayName": "Rohit Khattar",
      "userId": "03191352284834786517"
     },
     "user_tz": -330
    },
    "id": "lQg8GXN3LUvd",
    "outputId": "44cf8dd9-102e-4b14-fb7a-780851ec9c6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PDF with 15 pages\n",
      "Split documents into 52 chunks\n",
      "Vector store created successfully\n"
     ]
    }
   ],
   "source": [
    "pdf_url = \"https://arxiv.org/pdf/1706.03762.pdf\"  # Example working PDF - Attention is all you need !!\n",
    "  # Input the PDF URL\n",
    "\n",
    "\n",
    "# Process the document\n",
    "documents = load_pdf(pdf_url)\n",
    "vector_store = create_vector_store(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrfW5gQGLUvd"
   },
   "source": [
    "## Step 3: Initialize Groq LLM\n",
    "\n",
    "We'll set up the Groq model for generating responses using the deepseek-r1-distill-llama-70b model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739620032885,
     "user": {
      "displayName": "Rohit Khattar",
      "userId": "03191352284834786517"
     },
     "user_tz": -330
    },
    "id": "dvQnjVBKLUvd",
    "outputId": "9bccd6a6-7d43-4dc7-b4b9-eff739182aa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq LLM initialized.\n"
     ]
    }
   ],
   "source": [
    "def init_llm():\n",
    "    \"\"\"Initialize the Groq LLM.\"\"\"\n",
    "    return ChatGroq(\n",
    "        temperature=0.0,  # Lower temperature for focused responses\n",
    "        model_name=\"deepseek-r1-distill-llama-70b\"\n",
    "    )\n",
    "\n",
    "llm = init_llm()\n",
    "print(\"Groq LLM initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "immMIDtOLUve"
   },
   "source": [
    "## Step 4: Create the Question-Answering Function\n",
    "\n",
    "This function:\n",
    "1. Takes a user question\n",
    "2. Retrieves relevant context from the vector store\n",
    "3. Generates a response using the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kYWr639QLUve"
   },
   "outputs": [],
   "source": [
    "def get_answer(question: str, vector_store: FAISS, llm: ChatGroq) -> str:\n",
    "    \"\"\"Get an answer to a question using RAG.\"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    relevant_docs = vector_store.similarity_search(question, k=4)\n",
    "    print(f\"Found {len(relevant_docs)} relevant documents\")\n",
    "\n",
    "    # Assemble context\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "def get_answer(question: str, vector_store: FAISS, llm: ChatGroq) -> str:\n",
    "    \"\"\"Get an answer to a question using RAG.\"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    relevant_docs = vector_store.similarity_search(question, k=4)\n",
    "    print(f\"Found {len(relevant_docs)} relevant documents\")\n",
    "\n",
    "    # Assemble context retrieved from Vector Store\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = f\"\"\"Answer the question based only on the following context:\\n\\n\"\"\"\n",
    "    prompt += f\"Context:\\n{context}\\n\\n\"\n",
    "    prompt += f\"Question: {question}\\n\\n\"\n",
    "    prompt += \"Answer the question concisely and accurately. If you cannot answer the question based on the context, say \\\"I cannot answer this question based on the provided context.\\\"\\n\\n\"\n",
    "    prompt += \"Answer:\"\n",
    "\n",
    "    # Get the response from the LLM\n",
    "    response = llm.invoke(prompt)\n",
    "    return response\n",
    "    prompt += \"Answer the question concisely and accurately. If you cannot answer the question based on the context, say \\\"I cannot answer this question based on the provided context.\\\"\\n\\n\"\n",
    "    prompt += \"Answer:\"\n",
    "\n",
    "    # Get the response from the LLM\n",
    "    response = llm.invoke(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-ejCd3uLUve"
   },
   "source": [
    "## Step 5: Try It Out!\n",
    "\n",
    "Now you can ask questions about your PDF document. Let's try a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "executionInfo": {
     "elapsed": 2418,
     "status": "ok",
     "timestamp": 1739620035299,
     "user": {
      "displayName": "Rohit Khattar",
      "userId": "03191352284834786517"
     },
     "user_tz": -330
    },
    "id": "dkThzhHDLUve",
    "outputId": "36b7a8c1-9ed7-4602-f5cb-b087b4123ee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 relevant documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"border:1px solid #ccc; padding:15px; margin:15px 0; border-radius:8px; max-height:500px; overflow:auto; font-family:Arial;\">\n",
       "        <h2 style=\"margin-top:0; color:#333;\">Question:</h2>\n",
       "        <p style=\"font-size:16px;\">What are the key findings discussed in the document?</p>\n",
       "        <hr style=\"border:1px solid #eee;\">\n",
       "        <h2 style=\"margin-top:0; color:#555;\">Thinking Process:</h2>\n",
       "        <pre style=\"background:#f9f9f9; border:1px solid #ddd; padding:10px; border-radius:5px; font-size:14px; white-space:pre-wrap;\">Okay, I need to figure out the key findings discussed in the document based on the provided context. Let me go through the context step by step.\n",
       "\n",
       "First, I see that the context includes several sections. There are repeated lines about the law not being perfect but its application should be just, which seems to be a quote or a repeated statement. Then there are figures mentioned, like Figure 4 and Figure 3, which discuss attention heads in layer 5 of a neural network model. These figures talk about attention mechanisms, specifically how different attention heads focus on certain words or dependencies.\n",
       "\n",
       "Looking further, there are references to academic papers, such as Vinyals & Kaiser et al. (2015), Wu et al. (2016), Zhou et al. (2016), Zhu et al. (2013), and others. These papers are about neural machine translation, recurrent models, and parsing techniques. This suggests that the document is discussing advancements or analyses in neural network architectures, particularly focusing on attention mechanisms and their role in processing language.\n",
       "\n",
       "The figures and the text around them explain how attention heads in layer 5 are involved in resolving anaphoras and handling long-distance dependencies. For example, Figure 4 shows that attention heads 5 and 6 focus sharply on the word 'its', indicating how the model resolves references. Figure 3 demonstrates the model attending to dependencies related to the verb 'making', showing how different heads track these dependencies across the sentence.\n",
       "\n",
       "The repeated quote about the law might be a red herring or perhaps a test, but the main technical content is about the attention mechanisms in neural networks. The key findings would therefore relate to how these attention heads function, their focus on specific words, and their ability to handle grammatical structures like anaphoras and long-distance dependencies.\n",
       "\n",
       "So, putting it together, the key findings are about the analysis of attention mechanisms in a specific layer of a neural network model, detailing how they process certain words and dependencies, which is crucial for understanding the model's language processing capabilities.</pre>\n",
       "        <hr style=\"border:1px solid #eee;\">\n",
       "        <h2 style=\"margin-top:0; color:#555;\">Final Answer:</h2>\n",
       "        <pre style=\"background:#e8f4ff; border:1px solid #cce7ff; padding:10px; border-radius:5px; font-size:14px; white-space:pre-wrap;\">The key findings discussed in the document are about the analysis of attention mechanisms in a neural network model, specifically in layer 5. The attention heads in this layer were found to be involved in resolving anaphoras and handling long-distance dependencies. For instance, attention heads 5 and 6 focused sharply on the word 'its', demonstrating their role in anaphora resolution. Additionally, the model effectively tracked dependencies related to the verb 'making', showcasing how different attention heads manage grammatical structures. These findings highlight the model's ability to process complex linguistic elements through its attention mechanisms.</pre>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_rag_output(question, response):\n",
    "    # Check if the response contains a thinking process section\n",
    "    if \"<think>\" in response.content and \"</think>\" in response.content:\n",
    "        think_text = response.content.split(\"<think>\")[1].split(\"</think>\")[0].strip()\n",
    "        if \"Answer:\" in response.content:\n",
    "            answer_text = response.content.split(\"Answer:\")[-1].strip()\n",
    "        else:\n",
    "            answer_text = response.content.split(\"</think>\")[-1].strip()\n",
    "    else:\n",
    "        think_text = \"\"\n",
    "        # Access the 'content' attribute of the AIMessage object\n",
    "        answer_text = response.content.strip()\n",
    "\n",
    "\n",
    "    html_content = f\"\"\"\n",
    "    <div style=\"border:1px solid #ccc; padding:15px; margin:15px 0; border-radius:8px; max-height:500px; overflow:auto; font-family:Arial;\">\n",
    "        <h2 style=\"margin-top:0; color:#333;\">Question:</h2>\n",
    "        <p style=\"font-size:16px;\">{question}</p>\n",
    "        <hr style=\"border:1px solid #eee;\">\n",
    "        <h2 style=\"margin-top:0; color:#555;\">Thinking Process:</h2>\n",
    "        <pre style=\"background:#f9f9f9; border:1px solid #ddd; padding:10px; border-radius:5px; font-size:14px; white-space:pre-wrap;\">{think_text}</pre>\n",
    "        <hr style=\"border:1px solid #eee;\">\n",
    "        <h2 style=\"margin-top:0; color:#555;\">Final Answer:</h2>\n",
    "        <pre style=\"background:#e8f4ff; border:1px solid #cce7ff; padding:10px; border-radius:5px; font-size:14px; white-space:pre-wrap;\">{answer_text}</pre>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html_content))\n",
    "\n",
    "# Assuming you already have your vector_store and llm initialized\n",
    "questions = [\n",
    "    #What is the main topic of this document?\"#,\n",
    "    \"What are the key findings discussed in the document?\"#,\n",
    "    #\"Can you summarize the conclusion?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    response = get_answer(question, vector_store, llm)\n",
    "    show_rag_output(question, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqpMb9e5LUvf"
   },
   "source": [
    "## Understanding the Flow\n",
    "\n",
    "1. **Document Processing**:\n",
    "   - PDF is loaded and split into pages\n",
    "   - Text is chunked into smaller pieces\n",
    "   - Each chunk is converted to embeddings\n",
    "\n",
    "2. **Vector Store**:\n",
    "   - Embeddings are stored in FAISS for fast similarity search\n",
    "   - Retrieves relevant context for the given question\n",
    "\n",
    "3. **Question Answering**:\n",
    "   - The question is processed, and context is formed\n",
    "   - The LLM generates an answer based on the context\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Experiment with different chunk sizes\n",
    "- Adjust the number of retrieved documents (k value)\n",
    "- Modify prompt templates and temperature settings to optimize responses\n",
    "\n",
    "## Common Issues and Solutions\n",
    "\n",
    "1. **API Key Errors**: Ensure your Groq API key is set correctly\n",
    "2. **Memory Issues**: Adjust chunk sizes or number of retrieved documents if errors occur\n",
    "3. **Poor Responses**: Fine-tune the prompt or increase the retrieved context\n",
    "\n",
    "Happy exploring!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
